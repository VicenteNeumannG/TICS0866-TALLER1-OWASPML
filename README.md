# TICS0866-OWASPML
# Resumen Ejecutivo — *International AI Safety Report 2025*

El *International AI Safety Report 2025* es el primer informe internacional sobre la seguridad de la inteligencia artificial avanzada, elaborado con la colaboración de más de 90 expertos y respaldado por más de 30 países y organismos internacionales.  

El documento se centra en la **IA de propósito general**, es decir, modelos capaces de realizar múltiples tareas. Señala que estas tecnologías han progresado con gran rapidez y que su desarrollo futuro es incierto: podría avanzar de forma lenta o extremadamente rápida.  

Entre las conclusiones principales destacan:  
- **Capacidades en rápido crecimiento**: los modelos más avanzados ya superan a humanos en programación, matemáticas y razonamiento abstracto.  
- **Riesgos de mal uso**: estafas, deepfakes, delitos cibernéticos y potenciales aplicaciones en biotecnología.  
- **Riesgos de mal funcionamiento**: sesgos, falta de fiabilidad y posibilidad de pérdida de control.  
- **Riesgos sistémicos**: impacto en el empleo, concentración de poder económico, efectos medioambientales y amenazas a la privacidad.  
- **Mitigación aún incipiente**: técnicas como entrenamiento adversario, privacidad diferencial, evaluación y monitorización están en etapas tempranas.  
- **Necesidad de cooperación internacional**: el futuro de la IA no es inevitable; dependerá de las políticas y decisiones colectivas que se adopten hoy.  

El informe concluye que la IA puede generar beneficios significativos, pero que solo podrán materializarse si se gestionan adecuadamente los riesgos a nivel global.  

---

## Reflexión

Si bien el *International AI Safety Report 2025* ofrece una visión amplia de los riesgos estratégicos y sociales de la IA, **no aborda en detalle las vulnerabilidades técnicas específicas** que ya han sido documentadas en marcos como **OWASP ML Top 10** o **MITRE ATLAS**.  

Esta omisión refleja la distancia que existe entre el análisis de alto nivel y la práctica concreta de seguridad en sistemas de Machine Learning. Vulnerabilidades como la manipulación de entradas, el envenenamiento de datos, la extracción de modelos o los ataques de inferencia (OWASP ML-04 / MITRE AML.T0024) son ejemplos claros de cómo un modelo puede ser explotado en escenarios reales.  

La importancia de reconocer estas vulnerabilidades específicas radica en que constituyen el **puente entre la teoría y la práctica**: transforman la noción abstracta de “riesgo” en escenarios concretos, reproducibles y con impactos verificables sobre la privacidad, la integridad y la disponibilidad de los sistemas.  

En definitiva, para que la seguridad de la IA sea efectiva, los debates estratégicos deben incorporar estas vulnerabilidades técnicas. Solo así será posible traducir las recomendaciones generales en **planes de acción concretos**, que permitan proteger de forma real a los sistemas que procesan información crítica en todo el mundo.  
