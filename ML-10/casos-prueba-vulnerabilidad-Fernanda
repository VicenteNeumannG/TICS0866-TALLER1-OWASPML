1. Casos de prueba pentesting
## 游댮 CASO DE PRUEBA - [TIPO DE VULNERABILIDAD]

**ID:** TC-ML10-POISSOn-001
**Tipo:** ATAQUE
**Vulnerabilidad:** ML10
**Descripci칩n:** Manipulaci칩n del dataset con el cual entrenan sus modelos para insertar ejemplos maliciosos (que reconozca tr치fico malicioso como benigno), envenenando el modelo y generando falsos negativos en el tr치fico detectado.

**Entrada:** Registros de tr치fico del tipo "FUZZER", los cuales fueron etiquetados como normal cuando se entren칩 con el dataset de entrenamiento.
**Salida Esperada:** Que el modelo detecte de forma correcta el tr치fico como un ataque.
**Salida Real:** El modelo clasifica el tr치fico como tr치fico normal (o benigno)
**Estado:** COMPLETADO
**Severidad:** CR칈TICA

**Precondiciones:** 
- Tener acceso al dataset antes del entrenamiento
- Entrenamiento sin validaci칩n de integridad de los datos y sin supervisi칩n
- No hay t칠cnicas de robustez que permitan prevenir el ataque
**Postcondiciones:**
- El modelo tiene menor precisi칩n en ataques FUZZER
- Presenta un aumento de falsos negativos
- Riesgo de intrusiones no detectadas
**Observaciones:** Este tipo de ataque puede comprometer la seguridad del sistema sin ser detectado f치cilmente. Se recomienda implementar validaciones de calidad de datos y t칠cnicas de defensa.

**游댢 Procedimiento paso a paso:**
1. Obtener acceso al dataset
2. Identificar los registros con etiqueta "FUZZER"
3. Modificar una cierta cantidad de registros
4. Entrenar el 치rbol de decisi칩n con el dataset envenenado
5. Evaluar el modelo, tanto con conjunto de datos incorrectamente etiquetado (de forma intencional), con uno correctamente etiquetado.
6. Comparar el modelo, tanto antes como despu칠s del envenenamiento


## 游댮 CASO DE PRUEBA - ML10

**ID:** TC-ML10-POISSON-002
**Tipo:** ATAQUE
**Vulnerabilidad:** ML10
**Descripci칩n:** Insertar datos en venenados en el sistema, los cuales parecen leg칤timos pues que se encuentran bien etiquetados, con la finalidad de enga침ar al sistema, haci칠ndole creer que son datos bien clasificados.

**Entrada:** Datos que se encuentran mal etiquetados de forma intencional (Clean-label Attacks)
**Salida Esperada:** Que el modelo detecte que los datos de tr치fico, en verdad se trata de un ataque
**Salida Real:** Los datos con su etiqueta modificada, no son detectados por el sistema.
**Estado:** COMPLETADO
**Severidad:** CR칈TICA

**Precondiciones:**
- 
**Postcondiciones:**
- 
**Observaciones:** [NOTAS ADICIONALES]

**游댢 Procedimiento paso a paso:**
1. [PASO 1]
2. [PASO 2]
3. [PASO 3]
...
