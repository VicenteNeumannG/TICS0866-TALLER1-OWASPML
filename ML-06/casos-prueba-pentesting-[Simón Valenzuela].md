# Caso 1 – Dependency Confusion

- **Objetivo:** Mostrar cómo un atacante puede inyectar código malicioso aprovechando nombres de paquetes internos no protegidos.
- **Vector ML06:** Ataque a la cadena de suministro en dependencias de software.
- **Activo afectado:** Pipelines de CI/CD y entornos de desarrollo.
- **Métricas de impacto:** Ejecución de código no autorizado, pérdida de integridad en el pipeline.
- **Controles defensivos a evaluar:** Uso de repositorios privados autenticados, pinning de versiones, verificación de hashes.
- **Riesgos/ética:** Escenario simulado en laboratorio controlado, sin ejecución en entornos reales.

# Procedimiento detallado – Caso 1 (Dependency Confusion)

## Preparación
1. Identificar un paquete interno utilizado en un proyecto.
2. Crear un paquete con el mismo nombre pero alojado en un registro público.
3. Asignarle una versión superior a la usada en el entorno interno.

## Baseline
- El pipeline instala dependencias desde un repositorio privado sin incidentes.
- Evidencia esperada: listado de dependencias mostrando solo librerías internas.

## Ataque / Simulación
1. El pipeline resuelve dependencias y detecta una versión superior en el registro público.
2. Descarga e instala el paquete externo en lugar del interno.
3. El código malicioso se ejecuta durante la instalación.

## Evaluación y Resultados
- Evidencia esperada: logs de instalación mostrando descarga desde un registro externo.
- Evidencia esperada: ejecución de una acción no deseada al instalar la dependencia.

## Controles y Respuesta
- Pinning de versiones y hashes bloquea la instalación del paquete malicioso.
- Auditorías de dependencias identifican el paquete extraño.

## Conclusión
La falta de control sobre dependencias externas permite a un atacante inyectar código en el pipeline.

# Caso 2 – Data Poisoning en datasets públicos

- **Objetivo:** Explicar cómo un atacante puede modificar un dataset para afectar el desempeño de un modelo.
- **Vector ML06:** Contaminación de la cadena de suministro en la etapa de datos.
- **Activo afectado:** Dataset de entrenamiento y el modelo generado.
- **Métricas de impacto:** Caída en métricas de desempeño, aparición de sesgos o errores sistemáticos.
- **Controles defensivos a evaluar:** Validación estadística de datos, verificación de integridad mediante hashes, uso de datasets de referencia.
- **Riesgos/ética:** Caso descrito de manera teórica, sin manipulación real de datos externos.

# Procedimiento detallado – Caso 2 (Data Poisoning)

## Preparación
1. Seleccionar un dataset abierto utilizado para entrenamiento.
2. Identificar clases o etiquetas críticas en el conjunto de datos.

## Baseline
- El modelo entrenado con datos limpios muestra métricas estables.
- Evidencia esperada: accuracy y F1 en niveles normales.

## Ataque / Simulación
1. Alterar una parte del dataset, introduciendo etiquetas falsas o ejemplos maliciosos.
2. Entrenar nuevamente el modelo con el dataset contaminado.
3. Observar cambios en las métricas de desempeño.

## Evaluación y Resultados
- Evidencia esperada: comparación de métricas mostrando caída en accuracy/F1.
- Evidencia esperada: diff entre dataset limpio y contaminado.
- Evidencia esperada: salidas anómalas del modelo.

## Controles y Respuesta
- Verificación de integridad con hashes detecta la modificación.
- QA estadístico identifica distribuciones anormales en las etiquetas.
- Validación cruzada con datasets de control evidencia la degradación.

## Conclusión
El envenenamiento de datos puede pasar desapercibido si no se aplican controles de calidad e integridad.

