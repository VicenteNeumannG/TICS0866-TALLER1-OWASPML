# Casos de Prueba de Pentesting – ML03: Model Inversion Attack
**Autor:** [julian contreras]  
**Proyecto:** Intrusion.Aware (clasificador tipo Decision Tree sobre tráfico de red UNSW-NB15)  
**Alcance:** Diseño de pruebas (sin implementación ni explotación real).  
**Objetivo general:** Evaluar el riesgo de **inversión de modelo** (inferir/“reconstruir” información del conjunto de entrenamiento a partir de salidas del modelo).

---

## Caso 1 — Derivar “vectores arquetipo” por clase a partir de rutas del árbol
**Hipótesis.** En modelos interpretables (Decision Trees), un atacante puede recorrer umbrales/reglas para construir **vectores arquetipo** que describan combinaciones de atributos altamente representativas del entrenamiento para la clase *ATAQUE* o *NORMAL*. Esto **no** recupera registros 1:1, pero **sí** filtra estructura del dataset (perfilado de patrones de entrenamiento).

**Suposiciones/Precondiciones.**
- Acceso de consulta al modelo (endpoint o interfaz) que entrega etiqueta y, si existe, confianza/score.
- Conocimiento de que el modelo subyacente es un **árbol de decisión** (o acceso a sus reglas exportadas).

**Procedimiento paso a paso.**
1. **Enumerar rutas**: identificar (o inducir) distintas rutas del árbol modificando features clave (ej.: `proto`, `state`, `service`, `sbytes`, `dbytes`, `ct_srv_src`).
2. **Registrar hojas**: para cada entrada usada, anotar regla/umbrales activados, **hoja**, **clase predicha** y **confianza** (si está disponible).
3. **Seleccionar máximos**: localizar por clase las hojas con mayor confianza/soporte observado (más activaciones).
4. **Construir arquetipos**: sintetizar por clase un **vector arquetipo** con rangos/valores umbral que llevan de forma estable a esa hoja.
5. **Robustez local**: variar levemente los campos del arquetipo (±δ) y verificar que la clase/score permanezcan altos (indicio de región densa del entrenamiento).

**Evidencia esperada.**
- **Tabla de rutas** (reglas → hoja → clase → confianza/soporte).
- **Vectores arquetipo** por clase (con justificación de los umbrales críticos).
- **Breve análisis de riesgo**: por qué estos arquetipos sugieren fuga de estructura del set de entrenamiento (perfilado).

**Criterio de éxito / Pass-Fail.**
- *Pass*: se obtienen al menos 1–2 arquetipos por clase con estabilidad local (la predicción se mantiene ante pequeñas variaciones).
- *Fail*: no es posible identificar rutas/umbrales consistentes o la salida es demasiado ruidosa para sintetizar arquetipos.

**Riesgo y mitigación vinculadas.**
- Riesgo: filtración de **patrones** del entrenamiento que pueden ayudar a evasión o reconocimiento del modelo.
- Mitigaciones sugeridas: **limitar salidas** (solo etiqueta o scores discretizados), **rate-limiting** y **monitoreo** de consultas repetitivas.

---

## Caso 2 — Inferencia indirecta de atributos sensibles con barridos de probabilidad
**Hipótesis.** Si la API expone **probabilidades** por clase (o scores calibrados), es posible realizar **barridos sistemáticos** sobre subconjuntos de features “sensibles/latentes” para identificar **picos de confianza** que delaten regiones densas del entrenamiento (inversión parcial).

**Suposiciones/Precondiciones.**
- Endpoint de inferencia devuelve **probabilidades** / puntuaciones por clase.
- Se permite realizar múltiples consultas (sin bloqueo inmediato).

**Procedimiento paso a paso.**
1. **Definir base**: fijar un vector base válido (p. ej., `proto=tcp`, `state=ESTAB`, `sbytes=1200`, `dbytes=800`, `service=http`).
2. **Elegir subespacios**: seleccionar 2–3 features para barrido (ej.: `service`, `state`, `ct_srv_src`), manteniendo el resto fijo.
3. **Barridos univariados y cruzados**: variar los campos elegidos en rangos realistas y registrar **probabilidad(ATAQUE)** y **probabilidad(NORMAL)**.
4. **Detectar “picos”**: localizar combinaciones con **máximos locales** de confianza (umbrales concretos o combinaciones discretas).
5. **Reglas inversas**: formular reglas *si-entonces* del tipo “si (`service=x` y `state=y` y `ct_srv_src∈[a,b]`) ⇒ alta probabilidad de *ATAQUE*”, argumentando por qué esto sugiere **memorización local**.

**Evidencia esperada.**
- **Tabla o curva de sensibilidad**: feature(s) vs. probabilidad por clase.
- **Listado de entradas “gatillo”** con su score y explicación del patrón observado.
- **Breve análisis de fuga**: cómo estos picos reflejan regiones densas del entrenamiento.

**Criterio de éxito / Pass-Fail.**
- *Pass*: se identifican combinaciones con aumentos significativos y reproducibles de probabilidad.
- *Fail*: la probabilidad permanece plana/no informativa en el rango ensayado.

**Riesgo y mitigación vinculadas.**
- Riesgo: exfiltración de **estadística estructural** del dataset (regímenes donde el modelo es muy confiado).
- Mitigaciones: **reducir granularidad** de salida, **aleatorizar/ruidear** scores, **limitar tasa de consulta** y **auditar patrones de barrido**.

---

## Caso 3 — Medir estabilidad de inversión frente a perturbaciones controladas
**Idea.** Evaluar si pequeños cambios coordinados en features numéricas mantienen reglas/hojas y, por tanto, preservan la “reconstrucción” aproximada (indicio de sobreajuste local).

**Pasos resumidos.**
1. Partir de un vector arquetipo (del Caso 1 o 2).
2. Aplicar perturbaciones pequeñas simultáneas (±1–5%) en `sbytes`, `dbytes`, contadores `ct_*`.
3. Medir cuántas instancias **no cambian** de hoja/clase.
4. Reportar % de estabilidad y discutir riesgo.

**Evidencia.** Tabla con perturbación → hoja/clase/score; % de estabilidad.

---

## Consideraciones éticas y de alcance
- Estas pruebas evalúan **riesgos de inversión** sin buscar PII ni explotación real.
- No se ejecuta código ofensivo ni se vulneran sistemas externos; el foco es **diseño metodológico** y **conciencia de fuga de información** desde el endpoint de inferencia.


